Physics-informed neural networks (PINNs) have been developed as an alternative method to traditional numerical PDE solvers [44, 31]. PINNs solve PDEs by minimizing a loss function constructed from the PDEs, and the PDEs are solved when the loss is close to zero. Compared to traditional numerical solvers, a PINN is meshfree and thus can easily handle irregular-domain problems. Moreover, PINNs have been successfully employed to solve diverse inverse problems, including in optics [8], fluid mechanics [45, 55], systems biology [60], biomedicine [46, 24], and even inverse problems of stochastic PDEs [62] and fractional PDEs [38]. Here, the inverse problems were to infer unknown data (such as PDE coefficients/geometries) from partial observations of the PDE solutions in some regions, and a major concern is often regularizing the problem in order to ensure a unique “correct” solution. In contrast, for inverse design, one may not have any data of the PDE solution, and any realizable design that approximately maximizes the objective functional is acceptable (even if it is not unique).
For conventional numerical methods, inverse problems and inverse design might be solved by similar methods, because the PDE is directly solved via a numerical solver, and we only need to find the best PDE solution to minimize the objective function or match the observed data. However, in PINN, inverse design poses new challenges. The inverse problems are solved with PINNs by using the mismatch between the PDE solution and the data as a loss function (data-based loss), so that the network is trained to minimize the sum of the data-based loss and the PDE-based loss [8, 31]. Such an optimization problem can be solved relatively easily, because these two losses are consistent and can be minimized to zero simultaneously. In contrast, for inverse design, the PDE-based loss and the objective function are usually not consistent—it is not generally possible to both exactly solve the PDE and make the design objective arbitrarily good with the same solution—and so they compete with each other during optimization. Hence, a PINN that merely optimizes the sum of the objective and the PDE loss will usually end up in an optimum that is not a solution to the PDE. Moreover, inverse design problems often have additional inequality constraints, such from manufacturing constraints, that must be satisfied for the design to be acceptable.  To overcome these difficulties, we develop a new PINN method with hard constraints (hPINN) to solve PDE-constrained inverse design. We also consider inequality constraints in hPINN. We propose two approaches to impose the equality and inequality constraints as hard constraints, including the penalty method and the augmented Lagrangian method. We also use the approach of soft constraints for comparison. Imposing hard constraints to neural networks has been considered very recently [36, 9] for data-driven problems in the supervised learning paradigm.
