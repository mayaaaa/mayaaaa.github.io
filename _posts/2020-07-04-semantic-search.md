---
layout: post
title: Semantic Search in Millions of Equations
tags: [KDD20]
---

<!--more-->

### [PDF](https://dl.acm.org/doi/10.1145/3394486.3403056)
**イントロ**
- 関連論文の調査を行うための検索エンジンが必要
- 検索は文章や単語に基づくものが主
  - 技術用語は分野によって違う意味を持つことがある, 逆に違う単語でも分野によって同じ意味を持つことがある
  - 例えばBayes’ lawという単語は天体物理学では情報フィールド理論として知られている
  - 物理の知識がなくてもベイズの法則を思い浮かべることができる
  - 別の例では, イジングモデルは物理の論文ではFerromagnetismus, 初期はBoltzmann machineと呼ばれていた
  - イジングモデルの最初の論文はドイツ語, イェンセンの不等式の論文はフランス語
  - 一方数式は簡単に理解できる
  - 物理や情報系の論文検索には数式を使うべき
  - 数式同士の関連度を判断するため, ノーテーションの違いと数式の一部同士のマッチングを取る必要がある
- 提案手法では意味的な関連性を学習するためGCNを使う
- タスクを二つ設定
  - コンテクストの類似度に基づいた教師なし学習
  - マスクされた言語モデルにインスパイアされた自己教師あり
- arXivの90万の論文から28.9億の数式を抽出し数式をone-hot表現の特徴量を用いたグラフとして表現した
- 評価にはアノテーション付きのデータを使った

**提案手法**
- arXivの90万の論文から'equation', 'align'などLatexの数式環境を抜き出す (インラインは無視)
- Katexライブラリを使ってLaTeXの数式をXMLベースのMathML表現にコンパイル
- さらにcitation関係を抽出するため付録のarXiv-idを用いた
- MathMLをグラフに変換
  - グラフの葉ノードは数字, 括弧, ギリシャ文字などのテキスト, 
  - XML構造を木と見做しタグと文字に基づいて特徴量を抽出する
  - 特徴量は256次元のone-hot表現
    - 最初の32次元がタグの種類, 次の32次元が属性 (太字など), 最後の192次元は頻出文字 (図2)
    - 頻出でない文字と属性にはそれぞれそれ用のsymbolを用意
- 構築したグラフをGCNにつっこむ
- 類似度の定義
  - 同じ論文に出てくる数式, citation元とcitation先の論文に出てくる数式は意味的に似ていると見なす
  - さらに同じ論文, 同じ章に出てくる数式を区別する
- Triplet lossに基づいてGCNのパラメータを学習
  - (ステップ1) ランダムに論文をサンプリングし選んだ論文からさらに数式をサンプル (ステップ2) 同じ章, 同じ論文, 参照論文から数式をサンプル を繰り返してポジティブなサンプルを生成
  - (ステップ1)に加え (ステップ2)でランダムに論文をサンプリングし選んだ論文からさらに数式をサンプルすることでネガティブなサンプルを生成
  - 上記で生成したネガティブ/ポジティブなサンプルを用いてTriplet lossを計算
- 上記を拡張してもう一つのタスクを設定
  - 数式中のsymbolの一部だけ隠して学習し隠していた部分を再構築する
  - BERTで有名になったマスクされた言語モデルと似たタスク
  - 類似度タスクにこのタスクを追加することで文脈情報だけでなく生の入力symbolの情報を保存できる

**実験**
- まず機械学習分野のサブセットで手法を検証
  - 表1にRanking scoreの評価結果
  - 既存手法のBitmap CNNをわずかに下回る精度 <- 既存手法ではテストデータに同じ論文が含まれるためフェアな比較ではない
- 次に全てのデータを使って実験 (表2)
  - 提案手法は全てのデータを使って学習し機械学習分野の論文で評価
- 図4, 5にイントロの例に対応する定性評価を図示
  - 図4ではBayes’ lawの別の表現が見つかっている
  - 図5ではIsing ModelsとBoltzmann Machineの両方に関連したほぼ同等の数式が見つかった
  
