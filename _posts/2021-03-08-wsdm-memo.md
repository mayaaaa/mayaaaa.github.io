---
layout: post
title: "WSDM 2021 Memo"
tags: [WSDM21]
---

<!--more-->

国際会議WSDM2021の聴講メモ

[Program](https://www.wsdm-conference.org/2021/program-overview.php)

[Proceeding](https://www.wsdm-conference.org/2021/proceedings.php)


## WSDM2021 概要
- 今年の投稿数は603, 採択数は112 (採択率は18.6%)
- 史上2番目の投稿数
- 112本の採択論文のうち69本がオーラル
- ベストペーパーのファイナリスト
  - Online Experimentation with Surrogate Metrics: Guidelines and a Case Study
    Weitao Duan, Shan Ba, Chunzhe Zhang (LinkedIn Corporation)
   [PDF](https://dl.acm.org/doi/pdf/10.1145/3437963.3441737) 
  - RePBubLik: Reducing Polarized Bubble Radius with Link Insertions   
   [PDF](https://dl.acm.org/doi/pdf/10.1145/3437963.3441825)  
    Shahrzad Haddadan, Cristina Menghini, Matteo Riondato, Eli Upfal 
  - FACE-KEG: Fact Checking Explained using KnowledgE Graphs   
   [PDF](https://dl.acm.org/doi/pdf/10.1145/3437963.3441828)  
    Nikhita Vedula, Srinivasan Parthasarathy
  - Generative Models are Unsupervised Predictors of Page Quality: A Colossal-Scale Study 
   [PDF](https://dl.acm.org/doi/pdf/10.1145/3437963.3441809)   
    Dara Bahri, Yi Tay, Che Zheng, Cliff Brunk, Donald Metzler, Andrew Tomkins 
  - Unifying Online and Counterfactual Learning to Rank: A Novel Counterfactual Estimator that Effectively Utilizes Online Interventions   
   [PDF](https://dl.acm.org/doi/pdf/10.1145/3437963.3441794)  
    Harrie Oosterhuis, Maarten de Rijke



## キーノート
### Some thoughts on Computational Natural Language in the 21st century  
#### by Yoav Shoham
- ゲーム理論の研究者でスタンフォードの名誉教授であるYoav Shohamによる基調講演
- 自然言語分野における発展
  - モデル: BERT, GPTを含む深層学習モデルの興隆
  - データ: 大規模データセットの増加
  - タスク: 文章生成, 自然言語理解
- 精度だけでなく意味理解が重要になっている
  - 文章を読んで質問に答えるQ&Aタスクにおいては, 当初精度を問題にしていた
  - 最近, 根拠が間違っているが正しく予測されている ("right for the wrong reasons")ケースへの批判がある  
  - このような学習は一般化しにくいという実用上の問題もある
- 文章の生成はGPT-2やGroverといった手法によって達成されている
  - 精度はいいが根拠がないという批判がある
  - そのため, 時としてすごく単純な間違いをする
- AIは第3のフェーズへ
  - 人手による処理 (小規模データ+意味理解)→パターン学習 (大規模データ+意味理解なし)→新しいAI (大規模データ+意味理解)
- BERTに意味を加えたSenseBERTを提案 
  - BERTでI ate a [chiken]の予測をすると
    - lot, sandwitch, little, spoon等の結果を出力する
    - BERTは人がご飯を食べること, chikenが食べ物であることを知らない
  - SenseBERTはpre-training時に単語の意味を導入することでこの課題を解決する
  - 学習途中に「意味」を入力し, 予測時には単語だけでなく意味も出力する
  - 文脈に合った単語が出力できているかという評価でBERTを上回る精度
- PMI-Masking [ICLR 21]は別のアプローチ
  - 文章を意味のある塊ごとに分け, 塊全体をマスクする 
  - 2つの単語の共起を測るPMIをこの塊に対して拡張
    - PMIを複数単語に拡張する場合, 単純に拡張すると単語数が多いほどPMIが高くなる
    - PMIに含まれるlogをminで置き換えることで解決
    - これにより, 単なる頻度ではない測度としてPMIを定義できる
  - PMI-MaskingをBERTの精度が上がる
- HAIM: GPTにintentionを入れる
  - 文章生成モデルGPTを改善
  - 始点から終点までのパスを推定するのに意味と 
  - HAIMKは中間地点を指定
- wordtuneというアプリをリリース


### Harnessing big data for personalized medicine
#### by Eran Segal 

> 近年, 個々人の多様なデータ資源が利用可能になっている. 電子カルテデータに加え, ゲノミクス, トランスクリプトミクス等のオミクスのプロファイリング技術で生成したデータを用いて病気の発症を予測し, 原因を特定する機械学習技術を紹介する. 提案技術は, 肥満, 糖尿病, 心臓病の潜在的な要因について新たな洞察を与え, マイクロバイオーム, 代謝物, 免疫系レベルで数百の血液中バイオマーカーを同定する. これらの予測技術は, パーソナライズされた病気の予防・治療計画, さらにはメタボロームやマイクロバイオームに基づいた新しい治療法の開発に応用できる.  

- トピック1: Personalized nutrition project (個別化栄養プロジェクト)の一例 [Cell 15]
  - 食物に対する血糖値応答の予測・因子の特定を目指すプロジェクト
  - 500人以上の被験者の5万食以上の食事, 血糖値の変化を分析
  - その結果, 同じ食事を摂っていても, 個人によって血糖値応答が異なるということがわかった
  - 血糖値応用予測を個人化することで, 効果的な (血糖値の上昇を抑える)食事の提案が可能に
  - 被験者実験では, 予測アルゴリズムによる介入を行う群, 行わない群に被験者を分類し, それぞれで血糖値の変化を測定
  - 予測アルゴリズムによる介入を行った群では, 血糖値の急激な上昇を抑制することができた
    - 既存のアルゴリズムでは介入後の6ヶ月で改善が見られ, その後サチる
    - 提案アルゴリズムは全ての期間でより大きな改善が見られる
    - また, 12ヶ月を超えても改善効果が持続
 
- トピック2: 血中の代謝物濃度の予測 [NATURE 20]
  - 食事データからカフェインレベルを予測するアルゴリズムがある (決定木, 回帰モデルなど)
  - 食事, 薬, 属性など11の特徴量と腸内微生物叢 (マイクロバイオーム)の全ての組み合わせを考え, それらを血中の代謝物濃度の予測の共変量として使用 
  - 食事とマイクロバイオームが代謝物濃度の予測に効くということがわかった
  - 続いて, SHAPを使って特徴量の重要度を分析
  - コーヒーは血中のカフェイン濃度の予測に聞く
  - h-CMPFは魚の消費に影響される
  - また, 細菌が特定の代謝物濃度の予測に効くということが明らかに
  - この関係が因果かどうか確かめるため, 介入実験を行う
  - 20人の被験者を半分に分け, 半分にサワードウ (ライ麦パン)を食べてもらう
  - サワードウを摂取した被験者群は, 特定の代謝物質が増加することが判明

- トピック3: マイクロバイオームの一塩基多型 (SNP)解析
  - メタゲノム解析では, 細菌のゲノム混合物を直接読み出す
    - 各々の細菌について, ゲノムに違いを分析
  - 腸内微生物叢 (マイクロバイオーム)のデータが近年急速に増加
  - マイクロバイオームのSNPはBMIと相関がある
  - 1つの配列の位置でBMIが大幅に変わる
  - 患者にマイクロバイオームを投与する実験を行った
    - プラシーボ薬を2週間投与した後, ドナーから取得した細菌を6週間投与
    - マイクロバイオームの移植により, 症状を和らげることができた
  - 患者とドナーのマイクロバイオームの配列差異は, 投薬前は0.1~1%だった  
    →投薬実験後, この差が縮まった 



## オーラル

## Session 1: Society

### Population-Scale Study of Human Needs During the COVID-19 Pandemic: Analysis and Implications
#### Jina Suh (University of Washington, Microsoft Research) et al. 
- パンデミックかHuman needsへの影響
- Human needsはウェブ検索ログから
- Bingの検索クエリを分析
- マズローの欲求段階の定義に従って, human needsを手で分類 
  - Human needsを生理的欲求, 安全, 社会的欲求, 承認欲求, 自己実現の5つのカテゴリに分類 
  - 79のサブカテゴリを用意
- 検索ログに含まれる単語を, 先ほどの定義に従って5つのneedsのカテゴリに紐付け
  - Human needsが検索クエリに現れていると仮定
- 検索クエリ, クリックしたページの中で各カテゴリに含まれる単語を数える
- 各カテゴリに紐づいたクエリ数+クリック数 = human needsの大きさとして定量化
- 以下のリサーチクエスチョンに答える分析を実施 
  1. Human needsはパンデミック前後でどう変わるか
    - パンデミックにより, 全ての段階のneedsが高まっている
    - まずはトイレットペーパーなど日用品購入のニーズ (生理的欲求)が高まる
    - ロックダウン後, オンラインラーニングへの需要増加に伴って承認欲求が上昇
    - 続いて自己実現欲求 (料理レシピの検索, ページクリック)が上昇
    - 安全への欲求 (失業サイトへのアクセス)は急激な増加
  2. どのHuman needsが最も増加/低下したか
    - 生理的欲求, 安全が100倍以上増加 
    - サブカテゴリのうち, Wedding, Education, 仕事探しが低下
  3. 政治の長期的・短期的影響
    - 政府のメンタルヘルス関連サイトへの訪問が増加
  4. 検索クエリの分析は実際を正しく捉えられるか?
    - 失業サイトへのアクセス数は実際の失業率と同期
    - 一方, 家庭内暴力の報告数は増えているにもかかわらず, 検索クエリは低下  
      →被害者が自由にインターネットを使えない状況にある可能性
    - Human needsの分析を行う際は, 検索クエリ以外のデータソースも合わせて使うべき


### Ad Delivery Algorithms: The Hidden Arbiters of Political Messaging
#### Muhammad Ali (Northeastern University) et al. 
- 政治に広告が果たす役割は大きい
- 現在使われている広告アルゴリズムは自主的な倫理判断に依っている
- 古くはEqual-time-ruleがあるが, カバーしきれない問題が多くある
- 例えば, 共和党の広告と民主党の広告は広がり方が違う
  - トランプの広告よりサンダースの広告がシェラされやすい
- 他には, サンダースのファンにだけ投票ページを提示するというアルゴリズムも問題もある
- 政治広告を選択的に提示すべき

### Towards Ordinal Suicide Ideation Detection on Social Media
#### Ramit Sawhney (IIIT Delhi) et al.  
- 自殺数は全世界で増加
- 自殺の意思を持つ人はSNSを頻繁に使うことが知られている
- 本研究では, SNSの投稿から各ユーザの自殺リスクを評価するタスクに取り組む
- 先行研究は, 自殺リスク評価をバイナリ分類タスクとして定式化
  - このようなモデルは, 自殺リスクの"程度"を捉えることができない 
  - リスクのあるユーザ全てにフラグを立てることになるため, 医療リソースの逼迫につながる
- 最近の研究では, より詳細な自殺リスクを推定する手法が提案されている
  - ユーザの自殺リスクを連続値で出力
- 本研究では, 自殺リスク評価を順序回帰問題として定式化する
  - リスクレベルの単語を特定し, その数で心配な投稿を特定
  - リスクレベルの高い単語は先行研究の定義を用いる
  - Longformerで投稿文章をエンコード
  - 双方向LSTMで処理をしたあとaggregation
- 実験ではRedditデータを用いた定量評価・定性評価で提案手法の有用性を示した
  - 訂正評価では予測が難しいユーザに対しての自殺リスク評価結果を可視化している
  - 全ての手法で正しく予測できていないが, 提案手法が一番まし 


## Session 3: Recommender systems

### Popularity-Opportunity Bias in Collaborative Filtering
#### Ziwei Zhu (Texas A&M University) et al. 
- 推薦のためのモデルは多数提案されている
- 高精度な予測が可能になっている一方, バイアスへの配慮が問題になっている 
- 本研究では特に人気バイアス (popularity bias)にフォーカス
  - 人気の商品が不人気の商品より頻繁に推薦される
  - Rich-get-richer問題
- Popularity biasへの対処として, 人気の商品と不人気の商品の推薦の頻度を調整する手法がある
- しかし, 無理やり調整をかけると, そもそもの人気度の違いによる別のバイアスが発生する 
  - 同じ人数に不人気の商品, 人気の商品を推薦した場合, 人気の商品の方が購買確率が高くなる
- 本研究ではこのような問題に対処するため, ユーザと商品のマッチングの公平性を考慮した手法を提案する
- マッチするユーザに推薦されているかどうかを公平性の指標とする
- 人気バイアスをユーザ, 商品の両面から考える
- 商品の人気とマッチングしたユーザについての相関を取る
- 実験では, マッチングを考慮しない推薦手法と比較
- 提案手法は, 推薦の精度を保ちつつ人気バイアスを低減することができる


### A Black-Box Attack Model for Visually-Aware Recommenders
#### Rami Cohen (Intuit) et al. 
- 推薦は様々なサービスにおいて重要
  - Amazonにおける購買の35%, Netflixにおける閲覧の75%が推薦によるもの
- 画像を使った推薦がメジャーに
- このような推薦システムにおいては, 画像をエンコードして商品の潜在ベクトルとしてモデルに導入するアプローチが取られる
- 本研究では, 画像を用いた推薦におけるadversarial attackのリスクを定量的に評価する
  - 画像は企業から提供されているため, 恣意的に改変されているということも考えられる
- 以下の3つのシナリオでシミュレーションを行う
  1. 画像に小さな摂動 (ノイズ)を加えるケース
  2. パラメータの勾配に摂動を加える (White-box attack) 
  3. スコアに摂動を加える (Black-box attack)   
- 画像を使った推薦手法として, VBPRとdeepstyleを使用
  - 画像特徴量の抽出にResNetを使用
- 画像にノイズを入れても推薦の精度は大きく変化しない
- Black-box, white-boxアタックによって推薦の精度は落ちる
- 推薦手法の脆弱性を実データで示した
- この議論は, 画像以外の非構造化データにも適用することができる


### Leave No User Behind: Towards Improving the Utility of Recommender Systems for Non-mainstream Users
#### Roger Zhe Li, Julián Urbano, Alan Hanjalic (Delft University of Technology)
- 推薦に用いられる協調フィルタリングは類似性に基づく手法
  - 似たユーザは似た商品を買う
- この前提においては, 似たユーザがいないユーザ (非主流派)が不利になる
- すなわち, 非主流派については精度が下がる$$\leftarrow$$非主流派バイアスと呼ぶ
- 本研究では, 非主流派バイアスを和らげる推薦手法を提案
- 非主流派バイアスへの対策として, 以下の2通りが考えられる
  1. 商品のレビューなど, 内容についての特徴量を使う
  2. Adversarialの力を使う  
- 商品の内容を考慮する推薦手法として, DeepCoNN [WSDM 17]が有名
- 本研究では, Adversarialの力を使ってDeepCoNNを拡張
  - DeepCoNNにユーザの潜在ベクトル, 商品の潜在ベクトルの再構成ロスを加える
  - 具体的には, 強調フィルタリングのロス $$L_R$$と 再構成ロス $$L_U$$, $$L_I$$を足した $$L$$を目的関数として使用
- 実験では非主流派, 主流派のユーザグループそれぞれについて推薦の精度を評価
  - RMSEの値を元にユーザを4つのカテゴリに分け, 各々でRMSEを評価
- 3つのデータを使用
- ベースラインは行列分解とDeepCoNN
- 提案手法は主流派ユーザへの推薦の精度を下げることなく非主流派への推薦の精度を改善することができる
 

### Real-time Relevant Recommendation Suggestion
#### Ruobing Xie, Rui Wang, Shaoliang Zhang, Zhihong Yang, Feng Xia, Leyu Lin (WeChat Search Application Department, Tencent)
- SNSにおける投稿の閲覧において, ユーザは1つのトピックを連続的に見る傾向がある
  - 猫の画像見る人は猫ばかり見る
- 推薦システムは, 全てのトピックの投稿を全て見せるものが多い
- 本研究では, 閲覧中の投稿に関連する投稿を提案するタスクに取り組む
- 本タスクをrelevant recommendation suggestionと呼ぶ
- 提案手法は, 閲覧中の投稿のトピックをクエリとして用い, 検索で出てきた関連投稿を推薦する
- 検索をリアルタイムに行う
- 複数の要素を融合して手法の構築を行う
  - CTRを目的関数として用いる 
  - 投稿と関連投稿の意味的類似度を元にランク付けを行う
  - 推薦のタイミングを逸するとクリック率が下がる可能性がある  
    →検索とランキングによる遅延を考慮する 
- 提案手法M3oEはitem recommenderとbox triggerからなる
  - Item recommenderは投稿をクエリとして使い, 関連投稿を検索
    - Multi-head attention, SimNetを使用して関連投稿の類似度をモデル
  - 関連投稿を関連度順にランク付け
  - Box triggerは並び替えられた投稿のセット (box)を提示するかどうか判断
- WeChatデータを用いて実験
- 投稿の推薦の精度が向上



## Session 4: Networks

### RePBubLik: Reducing Polarized Bubble Radius with Link Insertions
#### Shahrzad Haddadan (Brown University) et al.  
- 社会ネットワークにおけるpolarization (両極性)を測る指標は2種類
  - 各個人の平均的な意見からの乖離
  - 異なる意見へのexposure
- 情報ネットワークにおける分析 (先行研究)
  - Wikipediaのリンクはユーザを一つの側のトピックに誘導しがち
  - Youtubeの推薦はユーザの軌跡を尖鋭化させる
- 情報ネットワークは, Webページをノード, エッジをWebページ間のハイパーリンクとしたネットワーク
- 各々のノード (Webページ)は異なる意見を持つ
- ユーザはクリックをすることで連続的にWebページを遷移
- 遷移の中で異なる色のWebページを経由する軌跡両極性が低い, 同じ色のページばかり遷移した軌跡は両極性が高いと見做せる
- Webページの閲覧履歴に基づいて情報ネットワークの両極性を測る測度を提案
  - 各々のページに対して, 頂点がどの程度反対色のWebページと繋がっているか測る
- Bounded random walkに基づくbubble radius (BR)という指標を提案
  - ページ $$v$$について, $$\overline{C_v}$$を $$v$$と反対の色を持つノードの集合とする
  - Bubble radiusを反対色のノードに達するまでに必要なランダムウォークのステップ数の期待値 $$B_G(v)=\mathbb{E}[v\rightsquigarrow\overline{C_v}]$$で定義
  - $$\rightsquigarrow$$はbounded ramdom walk (考慮するステップ数を $$t$$以下に制限)
- BRが $$t/2$$以上なら, そのノードは偏っているとみなす
  - 偏っているノードの色 $$C$$についての集合を $$\mathcal{P}(C)$$とおく 
  - 情報ネットワーク $$G$$の色 $$C$$についての偏り (構造的バイアス)を $$\sum_{v\in\mathcal{P}(C)}B_G(v)$$で評価
- 各々の色 $$C$$について、構造的バイアスを低減するためにエッジを追加するアルゴリズム RePBubLikを提案
  - 追加するエッジの数はあらかじめ与える
  - RePBubLikは貪欲法に基づく
- リンク追加前・追加後の構造的バイアスの差 $$\Delta$$を取り, これを最小化するような追加エッジ集合を推定
  - この差 $$\Delta$$が劣モジュラ性を持つことを証明
    - 貪欲法で求まる近似解により, 最悪の場合の解の品質がある程度の精度で理論的に保証されている
  - Random walkの中心性指標 (ノードの重要度を測る指標) $$\mathcal{R}$$を両極性の推定値に基づいて計算
  - $$\mathcal{R}$$を最大化する追加ノード $$v_i$$を探し, グラフに追加 
- 実験ではWikipedia, Amazonの書籍レビュー, 政治ブログの3つの情報ネットワークを使用
  - $$k$$つのエッジを追加した時の $$\Delta$$を調べた
  - 提案手法の簡易版の手法2つ+リンク追加のための手法2つと比較 
  - 同じ数のエッジを追加した場合, 提案手法の $$\Delta$$が最も大幅に改善


## Session 5: Experiments
### Online Experimentation with Surrogate Metrics: Guidelines and a Case Study 
#### Weitao Duan (LinkedIn) et al.  
- オンラインサービスやアプリケーションでは, 新しいアイデアが次々と生み出されている 
  - ここでアイデアとは, UIの変更, バックエンドのアルゴリズム, インフラなど
- アイデアを素早く評価し, その結果を理解する必要がある
- A/Bテスト (または制御実験)は, 意思決定のためのスタンダードとして, 広く採用されている
  - Microsoft, Google, Facebook, IinkedInは自社の実験プラットフォームを持っている
- 多くのシーンでは, アイデアの効果を正確に測定するのに時間がかかる
  - サブスクサービスについて, アイデアが顧客生涯価値 (LTV)にどう影響するかを評価したい
    - LTVはユーザがサブスクをやめるまでの短期間しか観測できない 
  - LinkedIn Job Marketplaceでは新機能や新推薦アルゴリズムが会員の就職支援の効果を向上させられるか評価したい
    - 新機能や新アルゴリズムは, 仕事のマッチング, 採用にすぐに影響を与えるが, 数ヶ月先まで影響が観測できない場合もある
- アイデアの効果を迅速に評価するため, 早期に観測できる変数に基づいて長期的なノーススター指標 (LTVや採用数など)を予測する方法がある
  - そのための機械学習モデルを構築し, このモデルによる予測を真のノーススターの近似値として使用する
- この近似値 (代用指標)は, A/Bテストの適用シーンにおいてはground truthとして直接用いられる
  - 予測モデルの不確実性は考慮されない
- この論文では, 通常のA/Bテストで代用指標による近似を行うと, 第一種過誤 (偽陽性, false positive)が大きくなる傾向があることを示した
- 実用時は, 代用指標の不確実性を考慮する必要がある
- 代理指標 $$S$$は, 統計的には $$P(Y=y\vert S,W)=P(Y=y\vert S)$$を満たす必要がある 
  - $$S$$で条件付けた場合, アイデア $$W$$と出力 $$Y$$が独立 
- 実験では履歴書から採用の有無を予測 


## Session 8: Web Analysis
### Generative Models are Unsupervised Predictors of Page Quality: A Colossal-Scale Study
#### Dara Bahri (Google Research) et al. 
- 自然言語生成モデルが広く用いられている
  - 文章生成, Q&A, 文章の意味理解に使用されている 
- 文章の "deepfake"の検出が重要なタスク
  - 人手による文章と自動生成された文章を区別する
- 例えば, 低品質なWebページの検出に役立つ
- 本研究では, deepfakeの検出器がWebページの品質を予測するのに役立つことを示した
  - Webページの品質を低品質, 中品質, 高品質の3段階に分類
  - 低品質は意味が通らない文章, 中品質は意味は通るが文法ミスを多く含む文章, 高品質は意味が通り文法も正しい
- WebページのデータWeb500Mから低品質なページを検出し, 分析を行った
- 文章生成モデルGPT-2が公開された2019年2月に低品質なWebページが増加
- 健康情報サイト, レポートを共有するサイトで低品質なWebページが多い
- 機械文章の検出器が予測に効くということを確認した


### Discovering Undisclosed Paid Partnership on Social Media via Aspect-Attentive Sponsored Post Learning
#### Seungbae Kim (University of California) et al. 
- インフルエンサーによる口コミでのマーケティングが流行っている
- スポンサーされている場合はそれを公開する必要がある
- ただし, スポンサー側もインフルエンサー側もこのルールを理解していないということが調査で判明している
  - スポンサー関係を隠すことで, ブランドイメージの低下, ユーザの満足度の低下につながる
- 本研究では, 隠されたスポンサーシップ関係を検出する
  - SNSの各投稿について, スポンサーされている度 (スポンサーシップスコア)を推定
- まず, インフルエンサー, 投稿, スポンサーという3種類のノードからなるグラフを構築 
  - 投稿のいいね数, インフルエンサーのフォロワー数等をそれぞれ各ノードの特徴量として使用
  - GNNをかけて各投稿の潜在ベクトルを得る
- 次に, 投稿に含まれる画像と文章をそれぞれエンコード
- グラフ, 画像, 文章からエンコードされた潜在ベクトルを組み合わせ, スポンサーシップスコアを回帰
- さらに, 各々のインフルエンサーは同じ時間帯に同じブランドに言及しがちという理由から,  
  スポンサーシップスコアは急に変わらないという制約をかける 
- 実験では, #ad, #sponsoredなどのタグが付いている投稿を教師付きデータとして使用
- スポンサーシップスコアのランク付けにおける精度で既存手法を上回る
- 公開されていないスポンサー関係を検出するタスクにおいても既存手法を上回る精度
- 分析では, グラフ, 画像, 文章の3種類の特徴量のうち, グラフと文章が特に重要であるということを確かめた
- また, スポンサー, インフルエンサー, 投稿のうち投稿の特徴量が特に重要
- 既存手法のBERTに比べ, 提案手法は特に短い文章からの検出が得意


### Quotebank: A Corpus of Quotations from a Decade of News
#### Timoté Vaucher (Ecole Polytechnique Fédérale de Lausanne)
- ニュースへのアクセスが手軽に
- 誤報, フェイクニュースが問題に
- ファクトチェックが必要
- 本研究では, ニュース記事などの文章から誰が何を言ったか (Who said what?)というファクトを抽出する
- 提案手法のQuatebankでは, 以下の3つの手順でWho said whatを表すデータを抽出
  1. 引用マークで囲まれた部分を抽出
  2. Wikiデータを使って名前を抽出
  3. 複数の人物が含まれる場合, 文章の構成を元にどちらかを選ぶ (attribution)
- 本研究では手順3 (attribution)にフォーカス
- Attributionのための手法は, 大きく2通り
  - 教師なし: ルールベースのブーストラッピング 
  - 教師あり: ラベル付きデータによる回帰   
    →複雑な文章の構造を捉えられる, アノテーションがいる
- 教師ありと教師なしを結びつける
  - ルールベースのブーストラッピングを使って学習データを生成
  - 生成したデータにデータ拡張を適用し, より複雑なデータを生成 
  - 生成したデータをtransformerベースの教師ありモデルに突っ込んで学習
- クラウドソーシングでアノテートした1500の引用データを用いて評価 
- 入力の文章を主語が2人以上いる文章, 不明確な文章, シンプルな文章の3種類に分類
- 提案手法は全種類の文章で提案手法を上回る精度
- 発言時刻を合わせて分析することで, ある人物のスタンスの変化を可視化できる
  - 応用の一部として, 政治家の政治的スタンスの変化を分析
  - とある政治家のトランプについての発言を検出し, 時系列に並べて可視化
  - 選挙の前後で発言が変わっているのがわかる 


### Contextualizing Trending Entities in News Stories
#### Marco Ponza (Bloomberg LP) et al. 
- Web上では, ニュース記事, Webページ, SNSの投稿など膨大なデータが蓄積されている
- このような情報に対して, 関連する情報を特定し, 理由とともに提示するツールが必要
- 本研究では, 特にtrending entityの"文脈付け"をゴールとする
  - 各々のtrending entityに対して, contextual entitiesのランク付を行う 
  - Contextual entitiesは, trending entityがなぜトレンドになっているのか説明するentity 
  - ここでは特に, ナレッジグラフのエンティティ=trending entityとする
- 提案手法では, contextual entitiesの候補を出して関連度順に並び替える 
- 教師ありの場合, 教師なしの場合の2つの設定下で定式化
- 教師なしの場合, Personalized pagerankを使用 
  - Wikipedia2edgeでエッジの重みを作成
- 教師ありの場合, 特徴エンジニアリング + ランク学習を使用
- クラウドソージングで集めたデータを使って実験
  - 140のtrending entityとそれぞれに関連づけられた120Kのcontextual entitiesを収集
- 教師あり・教師なし両方の設定下で既存手法を上回る精度
- 教師あり学習では, salience, 単語の共起が効くことを確かめた


## Session 10: Explainability and Intervention 

### Split-Treatment Analysis to Rank Heterogeneous Causal Effects for Prospective Interventions
#### Yanbo Xu (Georgia Institute of Technology) et al. 
- 推薦, 広告の提示など, オンライン上のユーザへの介入により, どのユーザが特に恩恵を受けるのか, 予測したい
  - アプリ等における追加機能の推薦は, スキルがあるユーザにとっては便利だが, そうでないユーザにとっては混乱の元になる
- 介入の効果に応じてユーザをランク付けするタスクに取り組む 
- 介入を $$Z$$, 効果を $$Y$$とおく
- 被験者を2つの群に分けABテストを行う
- 平均処理効果CATEは $$Z=1$$の介入をした処置群 $$\mathcal{G}\vert_{Z=1}$$としていない対照群 $$\mathcal{G}\vert_{Z=1}$$における効果 $$Y$$の差  
  $$\text{CATE} = \mathbb{E}_{ \mathcal{G}\vert_{Z=1} }[Y\vert Z=1] - \mathbb{E}_{ \mathcal{G}\vert_{Z=0} }[Y\vert Z=0]$$
- ここでは介入 $$Z$$が直接測定できないという仮定をおく
- $$Z$$の代わりにユーザの行動 $$A$$と観測可能な交絡因子 $${\bf X}$$を使う
- 行動 $$A$$が介入 $$Z$$の影響を全て吸収し, 効果 $$Y$$に影響すると仮定 
  - 行動 $$A$$を傾向スコアとして扱う 
- 行動 $$A$$に基づいて個人ごとの処理効果ITEを測る
- 効果 $$y_i$$, 交絡因子 $$x_i$$, 行動 $$a_i$$からなる系列 $$\{(y_i,{\bf x}_i,a_i)\}_{i=1}^n$$が与えられた下で, 行動 $$a$$と交絡因子 $$x$$で効果 $$y$$を回帰
- その際, IPTW法を用いて各データの損失関数に重みをつける
  - IPTW法は傾向スコアの逆数を用いて重み付けする手法
- 個人ごとの処理効果ITEを $$ITE^{(a)}=f({\bf x},a=1)−f({\bf x},a=0)$$で計算
- シミュレーションデータを使って実験
  - 推定されたユーザのランクと実際のランクとの間の平均平方根誤差 (RMSE)を計算
  - 提案手法では, RMSEが0.1と低くなっている
- 実際のソフトウェアにおける機能・製品の推薦に適用
  - 機能・製品について, クリックできるメッセージを出す
  - 推薦を行う前に, 提案手法を用いて, ソフトウェア製品の恩恵を受ける可能性の高いユーザを特定しておく
  - 既存のユーザの行動履歴を使用
  - ここで行動 $$A$$=ソフトウェアの利用, 介入 $$Z$$=ソフトウェアの試用を促すメッセージ, 効果 $$Y$$=ソフトウェアの持続的な使用
  - 220万人のユーザーのうち, 半分については提案手法で選んだユーザへの推薦を行い, 残りの半分については無作為に選んだユーザへの推薦を行った


### Explain and Predict, and then Predict Again
#### Zijian Zhang (L3S Research Center) et al.  
- 機械学習モデルの説明性, 透明性を担保した予測がしたい
- 本研究では, 特に自然言語処理のタスクを対象とする
- 説明可能な予測モデルとして, アテンションを使ったモデルが広く使われる 
- 単語の穴埋めタスクにおいては, evidence selecterを使った手法が提案されている
  - 単語系列のエンコードし, マスクと合わせて予測に使う
  - 学習がマスクに最適化されるという欠点がある
- 本研究では, 予測の根拠となる単語の推定 (ステップ1)をした後に, 根拠に基づいて予測を行う (ステップ2)フレームワークを提案する
  - ここで, 予測の根拠となる単語を推定=各単語に根拠となるかどうかバイナリ値を振る
  - ステップ1では根拠の推定をメインタスク, 予測を補助タスクとしてevidence selecterを学習
  - エンコーダはメインタスクと補助タスクで共有, デコーダは別
  - BERTを使ってevidence selecterを設計
  - ステップ2では, 補助タスクの出力を捨て, メインタスクの出力 (根拠)に基づいて本来の予測を行う
- Movie Reviews, FEVER, MultiRCの3つのデータセットを使って実験
- 実験で, 単語の予測タスクにおいて既存手法と同程度あるいは上回る精度を達成
- 説明の生成において既存手法を大きく上回る精度

### Combating Selection Biases in Recommender Systems with a Few Unbiased Ratings
#### Xiaojie Wang (Amazon.com, Inc.) et al.
- 推薦システムの選択バイアスに対処
- 2つの選択バイアスにより, 高評価が低評価より観測されやすくなる
  1. システムの商品選択: 予測評価の低い映画を推薦リストから除外 
  2. ユーザの評価: 好みでない商品に低評価をつけない
- このような問題に対して, 観測された各評価を傾向スコア (観測される確率)で逆に重み付けする方法が取られる 
- 本研究では, バイアスのない小さなデータセット使うアプローチを取る
- 既存のアプローチでは, 傾向モデルを学習した後, 学習済みの傾向スコアのパラメータを使って推薦モデルを学習する   
- 提案手法は, 傾向モデルと推薦モデルを交互に更新する
  - バイアス込みのデータセットで推薦モデルを学習しつつ, バイアスのない小さなデータセットで傾向モデルの性能を反復的に検証する
  - これにより, 汎化性能が保証される
- 不偏推定量を使う際, 評価モデルの更新時に安定した勾配を得るのが難しい
  - そこで, 傾向モデルの学習時に, 傾向推定値に関する正則化を入れる
- 2つのデータセットを用いて実験
  - 傾向スコアの評価で既存手法を上回る精度


## Session 12: Ranking 
### Unifying Online and Counterfactual Learning to Rank: A Novel Counterfactual Estimator that Effectively Utilizes Online Interventions
#### Harrie Oosterhuis
- バイアスを考慮したランキング学習 (LTR; Learning to Rank)
  - ランキング学習は, 多くの検索・推薦システムにおける基盤技術
- LTR法は人手によるアノテーションが主だったが, 近年, ユーザのインタラクション (クリック履歴)から学習を行うものが多くなっている
- ただし, クリックはユーザの嗜好以外の要因も反映しているため, これらを取り除く必要がある
- ユーザの嗜好以外の要因 (=バイアス)としては, 以下の3種類が考えられる　
  1. ポジション・バイアス：ユーザーは高ランクの商品をクリックしがち  
  2. 商品選択バイアス: ユーザは表示されていない商品をクリックすることができない 
  3. 信頼バイアス: 本来の嗜好と違う高ランクの商品をクリックしがち  
- LTR法は, 2つのアプローチに分けられる
  1. 反事実的アプローチ: バイアスをモデル化してクリック履歴から学習
  2. オンラインで介入を行うアプローチ: ユーザに提示するランキングに介入 
- 本研究では, 2つを組み合わせた手法を提案 
  - 反事実的LTR法であるaffine estimatorを拡張し, オンライン介入を考慮できる形に拡張する
- まず, 商品 $$d$$が $$k$$ランク目に提示された時にユーザがクリックする確率を $$P(C\vert d,k)=\alpha_k P(R=1\vert d)+\beta_k$$でモデル化
  - $$P(R=1\vert d)$$は商品の関連度
  - $$\beta_k$$は関連度に関係ないランクごとのクリックされやすさ
- 介入のポリシー $$\pi$$が与えられた時のクリックの期待値を上式を用いて記述 
- 全観測時間を通してのバイアスを考慮するため, 全ての時間におけるポリシー $$\{\pi_1,\pi_2,...\}$$について, クリックの期待値を計算
  - 既存の反事実的LTR法では, ポリシー $$\pi$$が時間を通して一定という仮定に基づいている
- この定義に基づき, バイアスを補正する関数 $$\hat{\Delta}$$を導く
- さらに, $$\hat{\Delta}$$に基づいて報酬 $$\hat{\mathcal{R}}$$を設計し, この報酬を最大化するポリシー $$\pi_t$$を学習 


## Session 11: Fairness 

### Towards Long-term Fairness in Recommendation
#### Yingqiang Ge (Rutgers University) et al. 
- Weiboの広告収入はアリババに偏っている
- この傾向は年々加速している
- こういった不公正性を考慮した推薦手法は多数提案されている
- 多くの手法は商品の人気が一定である, という仮定に依っている
- しかし, 実際には, 商品の人気は時間とともに変わる
- 商品の人気が変わらないという前提で公平性を課してしまうと, 長期的な不公正を招く可能性がある
- 例えば, 有名なマタイ効果は, 商品の人気度が遷移する場合は成り立たない 
- 本研究では, 商品の人気度の遷移を考慮した公平性の手法を提案
- 強化学習を使った手法を構築
  - Stateはユーザの状態
  - Actionは推薦商品のリスト
  - 報酬はクリックしたかどうか
  - コストは自由に設計できるが, 今回は人気のグループからのアイテム数に制約を課す
- コストが一定値を下回っていて, かつ, 報酬の蓄積が最大化されるようなポリシーを学習

 
## Session 13: Knowledge

### Temporal Cross-Effects in Knowledge Tracing
#### Chenyang Wang (Tsinghua University) et al. 
- 知識レベルの追跡は教育, adaptive learningの領域で重要なタスク
- 質問への回答の正解, 不正解に基づいて知識レベルをモデル化するアプローチが取られる
- Bayesian Knowledge Tracing (BKT), item response theory (IRT)等のモデルが広く用いられている
- 本研究では, スキル間の相互作用 (cross skill)を考慮する
- 時刻 $$t_i$$, 質問 $$q_i$$, 正解・不正解 $$a_i$$からなる相互作用 $$(q_i,t_i,a_i)$$の履歴が与えられた下で, 次の質問に正解するかどうかを予測する
- 多次元Hawkes過程ベースのモデルを提案
  - 知識レベルを表す強度関数を導入
  - $$i$$番目の相互作用 $${\bf x}_i$$と$$j$$番目の相互作用の関係 $${\bf x}_j$$をカーネル関数 $$\kappa_{ {\bf x}_i,{\bf x}_j }(\cdot)$$でモデル化
- 提案手法は既存手法より高精度かつ高速な学習が可能


### FACE-KEG: FAct Checking Explained using KnowledgE Graphs
#### Nikhita Vedula (The Ohio State University) et al. 
- ファクトチェックはフェイクニュースの検出など, 様々な応用で重要
- ファクトチェックのための手法は, 高精度で説明可能なモデルが望ましい
- 多くの手法はそれらの判断を説明できない
- 本研究ではとあるファクトの正誤の判断とともに自然言語による説明文を出力するモデルを提案
- ナレッジベースを文章, グラフに別々に変換
  - 文章をファクトしてモデルに入力
  - グラフを外部情報としてモデルに入力
  - LSTMとナレッジグラフに基づくcopy mechanismを使用
- それぞれニューラルネットでエンコードし, 2つの潜在ベクトルを得る
- 2つの潜在ベクトルを結合し, (i) ファクトの正誤の分類 (ii) 文章の生成 に使用
- 3つのデータを使用
  - 説明文は人手でアノテート 
- 提案手法は説明文を入力として使っていないにも関わらず, 既存手法と同等もしくはそれ以上の精度
  - 特にナレッジグラフが予測に有効であることを確かめた
  - ナレッジグラフによって多くのエンティティがカバーされているFEVERデータで特に良い精度
- 説明文の質についてユーザによる評価, 指標による自動での評価を行い, どちらも既存手法を上回る精度


### Learning Dynamic Embeddings for Temporal Knowledge Graphs
#### Siyuan Liao (Sun Yat-sen University) et al. 
- ナレッジグラフは2つのエンティティ $$s$$, $$o$$とそれらの関係 $$r$$の三つ組 $$(s,r,o)$$で表される
- 時間ナレッジグラフは, 三つ組に時刻 $$t$$の情報を加えた四つ組 $$(s_i,r_i,o_i,t)$$で表される
- Dynamic embeddingは各エンティティの潜在ベクトルの時間変化を捉える
- 過去のナレッジグラフの遷移に基づいて未来のナレッジグラフを予測
  - 例えば $$(s,r,?,t+1)$$の?を当てる
- エンティティを潜在空間にembeddingし, 潜在空間における距離でイベントの起こりやすさを予測するアプローチが取られる
- 三つ組の各要素の近さを測るのにTransEスコア関数が用いられている
- 各エンティティの潜在ベクトルの時間変化をマルコフモデルで記述
  - 時刻 $$t$$の状態 $$\epsilon_t$$が前の時刻の潜在状態 $$\epsilon_{t-1}$$に基づいて決まると仮定
  - 遷移確率をガウス分布で記述
  - ガウス分布の平均と分散の両方を $$\epsilon_{t-1}$$でモデル化
- 提案手法は, 事後分布の分母の積分を解析的に解くことができない
- 変分推論を使う
- オンライン・オフラインの設定を考え, それぞれパラメータの更新式を導出
- 2つのデータセットを使用
- リンク予測タスクにおける精度を評価
- 既存のembedding手法を上回る精度
- 各エンティティの潜在空間における距離を分析 (表5)
  - 関連が深いエンティティが潜在空間で近くにembeddingされている
 

## Session 14: Temporal Data and Forecasting

### Long Horizon Forecasting With Temporal Point Processes
#### Prathamesh Deshpande (Indian Institute of Technology Bombay) et al. 
- 短期間のイベントデータ $$(t_i,x_i,y_i)$$から長期の予測をする 
  - $$t_i$$はイベントの発生時刻, $$x_i$$は時刻 $$t_i$$の特徴量, $$y_i$$イベントの種類 
- 既存手法はRNNベースの点過程
  - RNNで過去のイベント系列をエンコード
  - RNNの出力を用いて点過程の強度関数を記述
- 長期予測をする場合は, foward samplingをする必要がある
  - 次のイベント発生時刻の予測を連続的に繰り返す
  - 予測時刻を入力として用い, さらに次の時刻の予測をする
  - これを繰り返すことで未来の時間帯におけるイベント系列を生成
- ただし, このようなアプローチは長期の予測で誤差が大きくなる
- 長期予測においては, 時間ビンごとに集計されたイベント"数" (カウント数)の予測が有効
- そこで, 点過程モデルの入力として, イベント系列だけでなく, カウント数を用いる手法を提案
  - 生のイベント系列とカウント数の整合性を取るため, 予測されたイベント系列のを集計したもの=カウント数の予測値 という制約を課す
- 尤度関数のうち, イベント系列に関する項が凹なので, 二次計画法で解ける
- Election (選挙に関するtweet), Taxi, Trafficデータを使って実験 
- 長期予測タスクで既存手法を上回る精度


### Sparse-Interest Network for Sequential Recommendation
#### Qiaoyu Tan (Texas AM University) et al. 
- 推薦システムは(i) 商品候補の生成→ (ii) ランキングの2段階 
  - (i)では強調フィルタリング が, (ii)では特徴量に基づいたランク付け が行われる
- 系列推薦 (sequential recommendation)がよく用いられる
  - 現在時刻までの購買履歴 (系列)を入力として, 次の購買を回帰
- 系列のembeddingには, transformerやCapsleなどのニューラルネットを使う 
  - Transformer: 商品のクラスタに制限をしない, 多様な興味を捉えられない
  - Casple: 少数のクラスタしか考慮しない, 多数の商品のカテゴリを考慮できない
- 本研究では, 多数のクラスタを考慮した系列のembedding手法を提案
- 各クラスタの特徴を表すコンセプト $$C$$ベクトルを導入
- 各ユーザ $$u$$に対して, 各コンセプトへの興味を表すプロトタイプ $${\bf C}_u$$を学習
  - ユーザの購買履歴をembeddingしたベクトル $${\bf z}_u$$と一般的なコンセプトのベクトル $$C$$の距離を計算
  - $$C$$のうち各ユーザベクトルと近いトップKの成分のみを抽出
  - $$K$$つの関数 $$\phi^k(\cdot)$$を用意し, それぞれにプロトタイプ $${\bf C}_u$$を突っ込む
- $$\phi^k(\cdot)$$の出力をaggregateし, ユーザベクトル $${\bf v}^u$$を得る
- ユーザベクトル $${\bf v}^u$$で次の購買商品 $${\bf x}_t$$を回帰
- 学習時は, コンセプトに関する正則化項を入れる
- 4つのデータセットで実験
- 系列推薦で使われる4つのベースラインと比較
- ほぼ全てのデータセット, 評価指標でベースラインを上回る精度
  - ユーザの興味を正確に学習できているため



### Time-Series Event Prediction with Evolutionary State Graph
#### Wenjie Hu (Alibaba Cloud) et al.

- 時系列からの代表的なパターンを抽出, 異常を検知するタスク
- 既存手法は, HMM, RNN, LSTMなど, 潜在状態を用いたモデル
- ただしこれらの手法は潜在状態間の関係とその時間発展を無視している
  - 潜在状態間の関係が時変しないという仮定を置いている
- 潜在状態と潜在状態間の関係を同時にモデル化する手法を提案
- 提案手法は3つのステップからなる 
  1. 時系列の分割&潜在状態の同定 (前処理)
    - 既存の時系列分割手法を適用し, 潜在状態と各々の時間変化パターンを得る
  2. 潜在状態間のグラフ構築
    - 潜在状態間の関係を各々の潜在状態に紐づいた時間変化パターンの距離で定義し, グラフを構築
  3. グラフの時間変化をモデル化 
    - 各セグメント時刻において, 潜在状態を表すグラフにMPNN (message passing neural network)をかけ, ノードの潜在ベクトルを更新 
    - ノードの潜在ベクトルの時間発展をLSTMでモデル化
    - 時刻 $$t$$におけるLSTMの潜在状態 $${\bf h}_G^{(t)}$$で次の時刻 $$t+1$$の異常 $$\hat{\bf Y}_{t+1}\in \{0,1\}$$を回帰
    - LSTMに注意機構を追加することで, 潜在状態の可視化が可能に
- この定式化は, 異常が起こっている時にグラフの構造が変化しているという仮定に基づく
- 株価, 機器故障, Webトラフィックのデータを使用 
- 異常検知タスクでグラフベース, 時系列ベースの既存手法を上回る精度
- アテンションを可視化することで, 異常を視覚的に提示できることを確かめた
- また, 状態潜在間のグラフを可視化すると, 異常状態への遷移確率が低く推定されていた 



