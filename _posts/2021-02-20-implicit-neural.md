---
layout: post
title: Implicit Neural Representations with Periodic Activation Functions 
tags: [NeurIPS20]
---

### [PDF](https://papers.nips.cc/paper/2020/hash/53c04118df112c13a8c34b38343b9c10-Abstract.html)
### [Video](https://www.youtube.com/watch?v=Or9J-DCDGko)

**アブスト**
- 画像, ビデオ, 点群, 音声等のシグナルのmapping $$\Phi$$をニューラルネットで記述する手法が提案されている
  - 例えば画像ピクセルの座標を入力したときに各ピクセルのRGB値を返す関数 $$\Phi$$をニューラルネットでモデル化
  - このような $$\Phi$$のパラメータを本稿では画像のimplicit representationと呼ぶ
  - この場合, 各ピクセル $${\bf X}_i$$のRGB $$f({\bf X}_i)$$と$$\Phi({\bf X}_i)$$の二乗誤差が最小になるような$$\Phi$$のパラメータを学習する
  - このような手法は画像を直接読み込むCNN等の手法と違ってグリッドの解像度に依存しない, メモリがシグナルの複雑さにスケールする, priorを考慮できる等のメリットがある
  - しかしRelu, tanh等の一般的なactivation functionを用いたMLPでは表現力に限界がある (位置普遍性の原則を満たさないのが一因)
  - また, Reluを使ったimplicit representationは1階微分がpiecewise constantに, 2階微分が0になる
    - 2階微分に意味がある場合これを利用することができない
    - 2階微分の非線形性を捉えることができない
- 提案手法はニューラルネット $$\Phi$$のactivation functionとしてsin関数を使い
- sin関数の特徴はその微分がcos関数 (すなわち位相をずらしたsin関数)で表せること
- この特徴を利用することでニューラルネットの出力そのものだけでなく入力 (画像の場合ピクセル座標)の1階微分, 2階微分についての学習もできるようになる 

**提案手法**
- 以下の制約を満たしながら $$\Phi$$のパラメータを最適化するタスクを考える
  - $$\mathcal{C}=\{{\bf x}, \Phi, \Delta_{\bf x}\Phi, \Delta^2_{\bf x}\Phi, ...\}=0$$
  - ここで$${\bf x}$$は座標, $$\Phi$$はニューラルネット
  - 本稿ではこのような表現 $$\Phi$$をimplicit neural representationと呼ぶ
  - このようなタスクは科学の幅広い分野で重要である
    - 符号付き距離関数を用いた三次元点群のモデル化, より一般的にはヘルムホルツ方程式, ポアソン方程式, 波動方程式などの境界値問題
- MLPのactivation functionを単純にsin関数で置き換え, それに合わせた初期化手法を提案する
- 具体的にはニューラルネット $$\Phi$$の $i$層目の変換を次式でモデル化
  - $$\phi_i({\bf x}_i)=\text{sin}({\bf W}_i{\bf x}_i+{\bf b}_i)$$
  - ここで $${\bf W}_i$$は重み行列, $${\bf b}_i$$はバイアスパラメータ
- このような定式化により, 提案手法の微分はそれ自身が提案手法の複合関数になる
  - sin関数の特徴はその微分がcos関数 (すなわち位相をずらしたsin関数)になるため
- 従ってニューラルネットの出力そのものだけでなくその微分についての学習も可能になる
- 図1は微分を使わないシンプルな画像系列へのfittingについて学習した例
  - ピクセルの座標 {\bf x}_iに対してRGB $$f({\bf x}_i)$$を回帰 
  - 一番上の列を見ると提案手法はRelu等を使った既存手法よりうまく詳細を再構築できている
  - さらに (学習の際は利用していないが)画像の微分を取れるというメリットもある
  - 図1の下2列にニューラルネットの1階微分, 2階微分を取った結果を示す
  - 前述の通り, Reluを使ったニューラルネットについては2階微分が0になっている
  - 提案手法は (学習の際は使っていないにも関わらず)画像のRGBそのものだけでなく1階微分, 2階微分についてもうまく再現できている
- さらにsin関数を用いたMLPのパラメータ初期化のための手法を提案

**実験**
- 4.1節: ポアソン方程式の学習
  - 画像のRGBそのものを使わず$$f({\bf x})$$の微分 $$\Delta_{\bf x}f({\bf x})$$う $$\Delta_{\bf x}\Phi({\bf x})$$を学習する 
  - あるいはラプラシアン (2階微分)をうまく再現するよう $$\Delta\cdot\Delta\Phi({\bf x})$$を学習
  - このような最適化を行う (ポアソン方程式を解く)ことで元の画像を再構成できる
    - 図3左上の左から二番目が微分を用いて学習した $$\Phi$$から再構成した元画像, 左から三番目がラプラシアンを用いた再構成 
    - 学習では微分あるいはラプラシアンのみを使っているにも関わらず, 元画像のRGBがうまく再構成できている
  - さらに画像は微分空間においてseamlessに結合できる
    - 図3右に微分領域で結合した画像の例を示す
    - 微分を用いることでseamlessな複合画像の生成が可能になる
- 4.2節: 符号付き距離関数を用いた点群の表現学習
　- 符号付き距離関数 (SDF)を点群にfitさせる
    - SDFは各点 $${\bf x}$$の境界面 $$\Omega$$からの距離
    - 各点についてのSDFの符号はその点が境界面のどちら側にあるかを示す
    - 点を入力としSDFの値をはくニューラルネットを学習
    - この問題は空間的微分 $$|\Delta_{\bf x}\Phi|$$が1という制約の下でEikonalの境界値問題を解くことに等しい
      - SDFは最寄りの点への距離なのでその微分はその点の逆方向を示す
      - その点から1単位空間ずれたらSDFは1大きくなる
      - 従って微分は1
    - この制約に加え, 表面の点集合 $$\Omega_0$$についてSDFが0に近づくような制約, SDFの微分とベクトルが平行になるような制約を(5)式第2項に追加
    - 式の最終項は表面以外の点集合 ($$\Omega \\ \Omega_0$$)が0に近くなるような制約 
    - Reluを用いたMLPと提案手法をこの目的関数を用いて学習
    - 図4を見ると提案手法は既存のReluを用いたMLPよりも詳細で複雑なシーンの再現が可能
- 4.4節: Implicit functionの空間を学習
  - ニューラルネットの学習空間にpriorを課す手法が提案されている
  - $$i$$層目のパラメータを $${\bf \theta}_i$$とおく
  - 全てのパラメータ $${\bf \theta}_i$$が $$k$$次元の部分空間に存在すると仮定する
  - 具体的にはパラメータ $${\bf \theta}_i$$を別のニューラルネットの出力 $${\bf z}_j$$で記述する
  - 別のニューラルネットはシグナル (の一部) $$O_j$$ (例えば画像の一部分)を入力とする
  - 図6は入力時に用いるピクセルの数を色々変えて元の画像を再構成したもの 


